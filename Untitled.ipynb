{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd27261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-docx) (4.9.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (9.5.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.1.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f62b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd81b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6938397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ''\n",
    "    for para in doc.paragraphs:\n",
    "        text += para.text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0253320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_resume(resume_text):\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    for line in resume_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # Check if the line represents a section header\n",
    "            if re.match(r'^[A-Z ]+$', line):\n",
    "                current_section = line.lower()\n",
    "                sections[current_section] = []\n",
    "            elif current_section:\n",
    "                sections[current_section].append(line)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e22fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca88e2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/232.6 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------- ------------------------- 81.9/232.6 kB 919.0 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/232.6 kB 939.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/232.6 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8af500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e40de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def is_relevant_line(line):\n",
    "    # Remove leading/trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # Check if the line is empty\n",
    "    if not line:\n",
    "        return False\n",
    "\n",
    "    # Check if the line is a header (e.g., \"Education\", \"Work Experience\")\n",
    "    if line.isupper() or line.istitle():\n",
    "        return True\n",
    "\n",
    "    # Check if the line starts with a bullet or number (likely a bullet point)\n",
    "    if line[0] in string.digits or line[0] in ['•', '-', '*']:\n",
    "        return True\n",
    "\n",
    "    # Check if the line contains keywords related to skills, education, or experience\n",
    "    keywords = ['skill', 'experience', 'degree', 'university', 'college', 'certification']\n",
    "    if any(keyword in line.lower() for keyword in keywords):\n",
    "        return True\n",
    "\n",
    "    # If none of the above conditions are met, consider the line irrelevant\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2629a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 25\n",
      "1. LinkedIn § GitHub  Portfolio # 21it3016@rgipt.ac.in\n",
      "2. Bachelor of Technology | Information Technology Dec 2021 - Jun 2025\n",
      "3. Rajiv Gandhi Institute of Petroleum Technology CPI: 9.12*\n",
      "4. Intermediate, 2021: 91.2% | Matriculation, 2019: 97.6%\n",
      "5. Cryto Tracker | ReactJS,Material UI,Coingecko API |  —\n",
      "6. ◦ Developed a React app utilizing the CoinGecko API to display real-time cryptocurrency price data, including profit/loss indicators and a line graph for visual analysis using react-chartjs .\n",
      "7. 3-D Sheet Customizer | ReactJS, Twailwind CSS, Three js, Node js, Express js |  —\n",
      "8. ◦ Created interactive 3-D web application with Three.js allowing users to customize shirt colors, logos, and textures, offering a\n",
      "9. TAPE-Movies | React,Twailwind CSS , TMDB API |  —\n",
      "10. ◦ Tape-Movies is a React-based movie website that allows users to search for movies by title, sort them by genre, view trending and upcoming movies using the TMDB api\n",
      "11. Weather Application | HTML,Twailwind Css, Javascript |  —\n",
      "12. ◦ Weather Application Developed using React to accurately predict and display real-time weather information like Temperature, humidity, wind speed for any city worldwide\n",
      "13. Random-Gif | React, Css, Javascript |  —\n",
      "14. GIFs based on user inputs and preferences.\n",
      "15. Engeria | HTML,Twailwind Css, Javascript |  —\n",
      "16. • Developed the offical website for the Sport fest of RGIPT\n",
      "17. Languages: C++, C, Python, SQL,HTML, CSS, JavaScript\n",
      "18. Tools/Frameworks: Git, GitHub, Flask, NodeJs, ReactJS, Django, Bettercap, Metasploit, WireShark\n",
      "19. Courses: Database Management, Data Structures, Algorithms, Networking, Web Technology, Cyber Security\n",
      "20. ◦ Responsible for Leading and teaching over 400 student fundamental of Cyber Security.\n",
      "21. ◦ Developed the official website for college sport fest ENERGIA 2\n",
      "22. ◦ Finalist of Rajasthan Police Hackathon 1.0\n",
      "23. ◦ Top 10 in Tri-NIT hackthon 2\n",
      "24. ◦ Achieved Rank 2 (Till date) in IT Department students in the CSE\n",
      "25. ◦ Achieved a top 2% rank in the IIT-JEE Advanced 2021 exam.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def remove_cid_patterns(line):\n",
    "    return re.sub(r'\\(cid:\\d+\\)', '', line)\n",
    "\n",
    "def merge_consecutive_sentences(lines):\n",
    "    merged_lines = []\n",
    "    current_line = \"\"\n",
    "    bullet_symbols = r'[\\u2022\\u2043\\u25E6\\u2666\\u2735\\u2736\\u2737\\u25CF\\u25AA\\u25AB\\u25A0\\u25A1\\u25B8\\u25D8\\u25E6\\u2713\\u2714\\u2718\\u2719\\u271D\\u2720\\u2721\\u2722\\u2723\\u27A2\\u2794\\u2798\\u27A8\\u27A9\\u25E6\\u25CB]'\n",
    "    for line in lines:\n",
    "        if line.strip()[0].isdigit() or line.strip()[0].isupper() or line.strip().startswith('—') or re.search(bullet_symbols, line.strip()):\n",
    "            if current_line:\n",
    "                merged_lines.append(current_line.strip())\n",
    "            current_line = line\n",
    "        else:\n",
    "            current_line += \" \" + line\n",
    "    if current_line:\n",
    "        merged_lines.append(current_line.strip())\n",
    "    return merged_lines\n",
    "\n",
    "def extract_lines_from_pdf(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Remove empty lines and (cid:XXX) patterns\n",
    "    text = remove_cid_patterns(text)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Split sentences into lines using newline character\n",
    "    lines = []\n",
    "    for sentence in sentences:\n",
    "        lines.extend(sentence.split('\\n'))\n",
    "\n",
    "    # Filter out short lines\n",
    "    lines = [line for line in lines if len(line.split()) > 5]\n",
    "\n",
    "    # Merge consecutive lines\n",
    "    merged_lines = merge_consecutive_sentences(lines)\n",
    "#     relevant_merged_lines = [line for line in merged_lines if is_relevant_line(line)]\n",
    "\n",
    "    return merged_lines\n",
    "\n",
    "# Usage\n",
    "file_path = 'Deepank_Resume.pdf'\n",
    "lines = extract_lines_from_pdf(file_path)\n",
    "\n",
    "# Print the total number of lines\n",
    "print(f\"Total number of lines: {len(lines)}\")\n",
    "\n",
    "# Print each line\n",
    "for i, line in enumerate(lines, start=1):\n",
    "    print(f\"{i}. {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40b3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 10\n",
      "1. Cryto Tracker | ReactJS,Material UI,Coingecko API |  —\n",
      "2. ◦ Developed a React app utilizing the CoinGecko API to display real-time cryptocurrency price data, including profit/loss indicators and a line graph for visual analysis using react-chartjs .\n",
      "3. 3-D Sheet Customizer | ReactJS, Twailwind CSS, Three js, Node js, Express js |  —\n",
      "4. ◦ Created interactive 3-D web application with Three.js allowing users to customize shirt colors, logos, and textures, offering a personalized virtual shopping experience.\n",
      "5. ◦ Tape-Movies is a React-based movie website that allows users to search for movies by title, sort them by genre, view trending and upcoming movies using the TMDB api\n",
      "6. ◦ Weather Application Developed using React to accurately predict and display real-time weather information like Temperature, humidity, wind speed for any city worldwide\n",
      "7. ◦ Finalist of Rajasthan Police Hackathon 1.0\n",
      "8. ◦ Top 10 in Tri-NIT hackthon 2\n",
      "9. ◦ Achieved Rank 2 (Till date) in IT Department students in the CSE\n",
      "10. ◦ Achieved a top 2% rank in the IIT-JEE Advanced 2021 exam.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_cid_patterns(line):\n",
    "    return re.sub(r'\\(cid:\\d+\\)', '', line)\n",
    "\n",
    "def merge_consecutive_sentences(lines):\n",
    "    merged_lines = []\n",
    "    current_line = \"\"\n",
    "    bullet_symbols = r'[\\u2022\\u2043\\u25E6\\u2666\\u2735\\u2736\\u2737\\u25CF\\u25AA\\u25AB\\u25A0\\u25A1\\u25B8\\u25D8\\u25E6\\u2713\\u2714\\u2718\\u2719\\u271D\\u2720\\u2721\\u2722\\u2723\\u27A2\\u2794\\u2798\\u27A8\\u27A9\\u25E6\\u25CB]'\n",
    "    for line in lines:\n",
    "        if line.strip()[0].isdigit() or line.strip()[0].isupper() or line.strip().startswith('—') or re.search(bullet_symbols, line.strip()):\n",
    "            if current_line:\n",
    "                merged_lines.append(current_line.strip())\n",
    "            current_line = line\n",
    "        else:\n",
    "            current_line += \" \" + line\n",
    "    if current_line:\n",
    "        merged_lines.append(current_line.strip())\n",
    "    return merged_lines\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            synonyms.append(synonym)\n",
    "    return synonyms\n",
    "\n",
    "def parse_resume(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Remove empty lines and (cid:XXX) patterns\n",
    "    text = remove_cid_patterns(text)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Split sentences into lines using newline character\n",
    "    lines = []\n",
    "    for sentence in sentences:\n",
    "        lines.extend(sentence.split('\\n'))\n",
    "\n",
    "    # Merge consecutive lines\n",
    "    merged_lines = merge_consecutive_sentences(lines)\n",
    "\n",
    "    # Parse the merged lines based on the specified criteria\n",
    "    parsed_lines = []\n",
    "    current_section = None\n",
    "    for line in merged_lines:\n",
    "        line = line.strip()\n",
    "        tokens = word_tokenize(line)\n",
    "\n",
    "        # Check for education section (including synonyms)\n",
    "        education_synonyms = ['education'] + get_synonyms('education')\n",
    "        if any(line.lower().startswith(section.lower()) for section in education_synonyms):\n",
    "            current_section = \"education\"\n",
    "            continue\n",
    "       \n",
    "\n",
    "        # Check for experience section (including synonyms)\n",
    "        experience_synonyms = ['experience'] + get_synonyms('experience')\n",
    "        if any(line.lower().startswith(section.lower()) for section in experience_synonyms):\n",
    "            current_section = \"experience\"\n",
    "            continue\n",
    "        elif current_section == \"experience\" and len(tokens) >= 10:\n",
    "            parsed_lines.append(line)\n",
    "\n",
    "        # Check for project section (including synonyms)\n",
    "        project_synonyms = ['projects'] + get_synonyms('projects')\n",
    "        if any(line.lower().startswith(section.lower()) for section in project_synonyms):\n",
    "            current_section = \"projects\"\n",
    "            continue\n",
    "        elif current_section == \"projects\" and len(tokens) >= 12:\n",
    "            parsed_lines.append(line)\n",
    "\n",
    "        # Check for skills section (including synonyms)\n",
    "        skills_synonyms = ['skills', 'technical skills', 'soft skills'] + get_synonyms('skills')\n",
    "        if any(line.lower().startswith(section.lower()) for section in skills_synonyms):\n",
    "            current_section = \"skills\"\n",
    "            continue\n",
    "\n",
    "        # Check for achievement section (including synonyms)\n",
    "        achievement_synonyms = ['achievements'] + get_synonyms('achievements')\n",
    "        if any(line.lower().startswith(section.lower()) for section in achievement_synonyms):\n",
    "            current_section = \"achievements\"\n",
    "            continue\n",
    "        elif current_section == \"achievements\" and len(tokens) >= 5:\n",
    "            parsed_lines.append(line)\n",
    "\n",
    "\n",
    "    # Filter out lines with less than 5 tokens\n",
    "    final_lines = [line for line in parsed_lines if len(word_tokenize(line)) >= 5]\n",
    "\n",
    "    return final_lines\n",
    "\n",
    "# Usage\n",
    "file_path = 'Deepank_Resume.pdf'\n",
    "parsed_lines = parse_resume(file_path)\n",
    "\n",
    "# Print the total number of lines\n",
    "print(f\"Total number of lines: {len(parsed_lines)}\")\n",
    "\n",
    "# Print each line\n",
    "for i, line in enumerate(parsed_lines, start=1):\n",
    "    print(f\"{i}. {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "157b0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 10\n",
      "1. Cryto Tracker | ReactJS,Material UI,Coingecko API |  —\n",
      "2. ◦ Developed a React app utilizing the CoinGecko API to display real-time cryptocurrency price data, including profit/loss indicators and a line graph for visual analysis using react-chartjs .\n",
      "3. 3-D Sheet Customizer | ReactJS, Twailwind CSS, Three js, Node js, Express js |  —\n",
      "4. ◦ Created interactive 3-D web application with Three.js allowing users to customize shirt colors, logos, and textures, offering a personalized virtual shopping experience.\n",
      "5. ◦ Tape-Movies is a React-based movie website that allows users to search for movies by title, sort them by genre, view trending and upcoming movies using the TMDB api\n",
      "6. ◦ Weather Application Developed using React to accurately predict and display real-time weather information like Temperature, humidity, wind speed for any city worldwide\n",
      "7. ◦ Finalist of Rajasthan Police Hackathon 1.0\n",
      "8. ◦ Top 10 in Tri-NIT hackthon 2\n",
      "9. ◦ Achieved Rank 2 (Till date) in IT Department students in the CSE\n",
      "10. ◦ Achieved a top 2% rank in the IIT-JEE Advanced 2021 exam.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import docx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_cid_patterns(line):\n",
    "    return re.sub(r'\\(cid:\\d+\\)', '', line)\n",
    "\n",
    "def merge_consecutive_sentences(lines):\n",
    "    merged_lines = []\n",
    "    current_line = \"\"\n",
    "    bullet_symbols = r'[\\u2022\\u2043\\u25E6\\u2666\\u2735\\u2736\\u2737\\u25CF\\u25AA\\u25AB\\u25A0\\u25A1\\u25B8\\u25D8\\u25E6\\u2713\\u2714\\u2718\\u2719\\u271D\\u2720\\u2721\\u2722\\u2723\\u27A2\\u2794\\u2798\\u27A8\\u27A9\\u25E6\\u25CB]'\n",
    "    for line in lines:\n",
    "        if line.strip()[0].isdigit() or line.strip()[0].isupper() or line.strip().startswith('—') or re.search(bullet_symbols, line.strip()):\n",
    "            if current_line:\n",
    "                merged_lines.append(current_line.strip())\n",
    "            current_line = line\n",
    "        else:\n",
    "            current_line += \" \" + line\n",
    "    if current_line:\n",
    "        merged_lines.append(current_line.strip())\n",
    "    return merged_lines\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            synonyms.append(synonym)\n",
    "    return synonyms\n",
    "\n",
    "def parse_resume(file_path):\n",
    "    text = ''\n",
    "    if file_path.endswith('.pdf'):\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + '\\n'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a PDF or Word file.\")\n",
    "\n",
    "    # Remove empty lines and (cid:XXX) patterns\n",
    "    text = remove_cid_patterns(text)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Split sentences into lines using newline character\n",
    "    lines = []\n",
    "    for sentence in sentences:\n",
    "        lines.extend(sentence.split('\\n'))\n",
    "\n",
    "    # Merge consecutive lines\n",
    "    merged_lines = merge_consecutive_sentences(lines)\n",
    "\n",
    "    # Parse the merged lines based on the specified criteria\n",
    "    parsed_lines = []\n",
    "    current_section = None\n",
    "    for line in merged_lines:\n",
    "        line = line.strip()\n",
    "        tokens = word_tokenize(line)\n",
    "\n",
    "        # Check for education section (including synonyms)\n",
    "        education_synonyms = ['education'] + get_synonyms('education')\n",
    "        if any(line.lower().startswith(section.lower()) for section in education_synonyms):\n",
    "            current_section = \"education\"\n",
    "            continue\n",
    "        \n",
    "        # Check for experience section (including synonyms)\n",
    "        experience_synonyms = ['experience'] + get_synonyms('experience')\n",
    "        if any(line.lower().startswith(section.lower()) for section in experience_synonyms):\n",
    "            current_section = \"experience\"\n",
    "            continue\n",
    "        elif current_section == \"experience\" and len(tokens) >= 10:\n",
    "            parsed_lines.append(line)\n",
    "\n",
    "        # Check for project section (including synonyms)\n",
    "        project_synonyms = ['projects'] + get_synonyms('projects')\n",
    "        if any(line.lower().startswith(section.lower()) for section in project_synonyms):\n",
    "            current_section = \"projects\"\n",
    "            continue\n",
    "        elif current_section == \"projects\" and len(tokens) >= 12:\n",
    "            parsed_lines.append(line)\n",
    "\n",
    "        # Check for skills section (including synonyms)\n",
    "        skills_synonyms = ['skills', 'technical skills', 'soft skills'] + get_synonyms('skills')\n",
    "        if any(line.lower().startswith(section.lower()) for section in skills_synonyms):\n",
    "            current_section = \"skills\"\n",
    "            continue\n",
    "\n",
    "        # Check for achievement section (including synonyms)\n",
    "        achievement_synonyms = ['achievements'] + get_synonyms('achievements')\n",
    "        if any(line.lower().startswith(section.lower()) for section in achievement_synonyms):\n",
    "            current_section = \"achievements\"\n",
    "            continue\n",
    "        elif current_section == \"achievements\" and len(tokens) >= 5:\n",
    "            parsed_lines.append(line)\n",
    "\n",
    "       \n",
    "\n",
    "    # Filter out lines with less than 5 tokens\n",
    "    final_lines = [line for line in parsed_lines if len(word_tokenize(line)) >= 5]\n",
    "\n",
    "    return final_lines\n",
    "\n",
    "# Usage\n",
    "file_path = 'Deepank_Resume.pdf'  # Replace with the actual file path\n",
    "# file_path = 'path/to/your/resume_file.docx'  # Replace with the actual file path\n",
    "\n",
    "parsed_lines = parse_resume(file_path)\n",
    "\n",
    "# Print the total number of lines\n",
    "print(f\"Total number of lines: {len(parsed_lines)}\")\n",
    "\n",
    "# Print each line\n",
    "for i, line in enumerate(parsed_lines, start=1):\n",
    "    print(f\"{i}. {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bf9bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting language_tool_python\n",
      "  Downloading language_tool_python-2.8-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pip in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language_tool_python) (24.0)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language_tool_python) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language_tool_python) (4.66.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language_tool_python) (0.40.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->language_tool_python) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->language_tool_python) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->language_tool_python) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->language_tool_python) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->language_tool_python) (0.4.6)\n",
      "Downloading language_tool_python-2.8-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: language_tool_python\n",
      "Successfully installed language_tool_python-2.8\n"
     ]
    }
   ],
   "source": [
    "! pip install language_tool_python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82edd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65b368fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled words:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     31\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRajiv Gandhi Institute of Petroleum Technology (RGIPT), Amethi, Uttar Pradesh 2021-2025\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mcheck_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 26\u001b[0m, in \u001b[0;36mcheck_spelling\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMisspelled words:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m misspelled_word \u001b[38;5;129;01min\u001b[39;00m misspelled_words:\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmisspelled_word[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmisspelled_word\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuggestions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo misspelled words found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "import language_tool_python\n",
    "import re\n",
    "\n",
    "def check_spelling(sentence):\n",
    "    # Initialize the spell checker\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    tokens = re.findall(r'\\w+', sentence)\n",
    "\n",
    "    # Check spelling of each token\n",
    "    misspelled_words = []\n",
    "    for token in tokens:\n",
    "        matches = tool.check(token)\n",
    "        spelling_errors = [match for match in matches if match.ruleIssueType.lower() == 'misspelling']\n",
    "        if spelling_errors:\n",
    "            misspelled_words.append({\n",
    "                'word': token,\n",
    "                'suggestions': [match.replacements for match in spelling_errors]\n",
    "            })\n",
    "\n",
    "    # Print the misspelled words and suggestions\n",
    "    if misspelled_words:\n",
    "        print(\"Misspelled words:\")\n",
    "        for misspelled_word in misspelled_words:\n",
    "            print(f\"- {misspelled_word['word']}: {', '.join(misspelled_word['suggestions'])}\")\n",
    "    else:\n",
    "        print(\"No misspelled words found.\")\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Rajiv Gandhi Institute of Petroleum Technology (RGIPT), Amethi, Uttar Pradesh 2021-2025\"\n",
    "check_spelling(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7e0cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting happytransformer\n",
      "  Downloading happytransformer-3.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: torch>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from happytransformer) (1.12.1)\n",
      "Requirement already satisfied: tqdm>=4.43 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from happytransformer) (4.66.2)\n",
      "Collecting transformers<5.0.0,>=4.30.1 (from happytransformer)\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/43.8 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.8/43.8 kB 714.4 kB/s eta 0:00:00\n",
      "Collecting datasets<3.0.0,>=2.13.1 (from happytransformer)\n",
      "  Using cached datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from happytransformer) (0.2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from happytransformer) (3.20.3)\n",
      "Requirement already satisfied: accelerate<1.0.0,>=0.20.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from happytransformer) (0.30.0)\n",
      "Collecting tokenizers<1.0.0,>=0.13.3 (from happytransformer)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: wandb in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from happytransformer) (0.12.21)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (21.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.10.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.7.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (1.4.4)\n",
      "Collecting requests>=2.32.1 (from datasets<3.0.0,>=2.13.1->happytransformer)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets<3.0.0,>=2.13.1->happytransformer) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.9.5)\n",
      "Collecting huggingface-hub (from accelerate<1.0.0,>=0.20.1->happytransformer)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.0->happytransformer) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.43->happytransformer) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.30.1->happytransformer) (2023.12.25)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (8.0.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (3.1.40)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (1.0.13)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (2.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (1.16.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb->happytransformer) (68.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from GitPython>=1.0.0->wandb->happytransformer) (4.0.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->accelerate<1.0.0,>=0.20.1->happytransformer) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.1->datasets<3.0.0,>=2.13.1->happytransformer) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.1->datasets<3.0.0,>=2.13.1->happytransformer) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.1->datasets<3.0.0,>=2.13.1->happytransformer) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.1->datasets<3.0.0,>=2.13.1->happytransformer) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2023.3.post1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->happytransformer) (5.0.1)\n",
      "Downloading happytransformer-3.0.0-py3-none-any.whl (24 kB)\n",
      "Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "   ---------------------------------------- 0.0/542.1 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 81.9/542.1 kB 4.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 204.8/542.1 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 471.0/542.1 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 542.1/542.1 kB 3.1 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/9.1 MB 7.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/9.1 MB 7.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/9.1 MB 6.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.1 MB 6.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/9.1 MB 6.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/9.1 MB 6.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.2/9.1 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/9.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.0/9.1 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/9.1 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.1 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.3/9.1 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.7/9.1 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.6/9.1 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.2/9.1 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.3/9.1 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.3/9.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.5/9.1 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.7/9.1 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.9/9.1 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.2/9.1 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.2/9.1 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.7/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.3/9.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.4/9.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.6/9.1 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/9.1 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.8/9.1 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 122.9/401.7 kB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 245.8/401.7 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  399.4/401.7 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 401.7/401.7 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: requests, huggingface-hub, tokenizers, transformers, datasets, happytransformer\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.10.1\n",
      "    Uninstalling datasets-2.10.1:\n",
      "      Successfully uninstalled datasets-2.10.1\n",
      "Successfully installed datasets-2.19.2 happytransformer-3.0.0 huggingface-hub-0.23.3 requests-2.32.3 tokenizers-0.19.1 transformers-4.41.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "allennlp 2.10.1 requires spacy<3.4,>=2.1.0, but you have spacy 3.7.5 which is incompatible.\n",
      "allennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 4.41.2 which is incompatible.\n",
      "cached-path 1.1.6 requires huggingface-hub<0.11.0,>=0.8.1, but you have huggingface-hub 0.23.3 which is incompatible.\n",
      "spacy-transformers 1.3.5 requires transformers<4.37.0,>=3.4.0, but you have transformers 4.41.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install happytransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8ab5f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Integrated a robust cipher algorithm for text encryption, ensuring 95% security, decreased vulnerability to cyber attacks by 50% and enhanced data protection compliance by 40%.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "\n",
    "args = TTSettings(num_beams=5, min_length=1)\n",
    "\n",
    "# Add the prefix \"grammar: \" before each input \n",
    "result = happy_tt.generate_text(\"• Integrated a robust cipher algorithm for text encryption, ensuring 95% security, decreased vulnerability to cyber attacks by 50% and enhanced data protection compliance by 40%.\", args=args)\n",
    "\n",
    "print(result.text) # This sentence has bad grammar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bda3828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar correction: Rajiv Gandhi Institute of Petroleum Technology (RGIPT), Amethi, Uttar Pradesh 2021-2025.\n",
      "Spelling correction: Rajiv Gandhi Institute of Petroleum Technology (RGIPT), Amethi, Uttar Pradesh, 2021-2025.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings\n",
    "from transformers import pipeline\n",
    "\n",
    "# For grammar correction\n",
    "happy_tt_grammar = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "args_grammar = TTSettings(num_beams=5, min_length=1)\n",
    "\n",
    "# For spelling correction\n",
    "fix_spelling = pipeline(\"text2text-generation\", model=\"oliverguhr/spelling-correction-english-base\")\n",
    "\n",
    "def check_grammar_and_spelling(text):\n",
    "    # Check for grammar errors\n",
    "    grammar_result = happy_tt_grammar.generate_text(f\"grammar: {text}\", args=args_grammar)\n",
    "    grammar_corrected = grammar_result.text.replace(\"grammar: \", \"\")\n",
    "    if grammar_corrected != text:\n",
    "        print(f\"Grammar correction: {grammar_corrected}\")\n",
    "    else:\n",
    "        print(\"No grammar errors found.\")\n",
    "\n",
    "    # Check for spelling errors\n",
    "    spelling_corrected = fix_spelling(text, max_length=2048)[0]['generated_text']\n",
    "    if spelling_corrected != text:\n",
    "        print(f\"Spelling correction: {spelling_corrected}\")\n",
    "    else:\n",
    "        print(\"No spelling errors found.\")\n",
    "\n",
    "# Example usage\n",
    "text = \"Rajiv Gandhi Institute of Petroleum Technology (RGIPT), Amethi, Uttar Pradesh 2021-2025\"\n",
    "check_grammar_and_spelling(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fc2c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gingerit in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install gingerit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8954ac87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gingerit.gingerit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgingerit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgingerit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgingerit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GingerIt\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_grammar_and_spelling\u001b[39m(text):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Initialize the GingerIt parser\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     parser \u001b[38;5;241m=\u001b[39m GingerIt()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gingerit.gingerit'"
     ]
    }
   ],
   "source": [
    "from gingerit.gingerit.gingerit import GingerIt\n",
    "\n",
    "def check_grammar_and_spelling(text):\n",
    "    # Initialize the GingerIt parser\n",
    "    parser = GingerIt()\n",
    "\n",
    "    # Check for errors\n",
    "    result = parser.parse(text)\n",
    "\n",
    "    # Check if there are any corrections\n",
    "    if result.corrections:\n",
    "        # Print grammar corrections\n",
    "        if result.corrections.get('grammar'):\n",
    "            print(\"Grammar corrections:\")\n",
    "            for correction in result.corrections['grammar']:\n",
    "                print(f\"- {correction['text']}\")\n",
    "\n",
    "        # Print spelling corrections\n",
    "        if result.corrections.get('spelling'):\n",
    "            print(\"Spelling corrections:\")\n",
    "            for correction in result.corrections['spelling']:\n",
    "                print(f\"- {correction['text']}\")\n",
    "    else:\n",
    "        print(\"No grammar or spelling errors found.\")\n",
    "\n",
    "# Example usage\n",
    "text = \"Hi, my name is Deepank Singh , i am a back-end devloper\"\n",
    "check_grammar_and_spelling(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e722201b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gingerit.gingerit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgingerit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgingerit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GingerIt\n\u001b[0;32m      3\u001b[0m st\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpell Corrector and Grammar Checker\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m text\u001b[38;5;241m=\u001b[39mst\u001b[38;5;241m.\u001b[39mtext_input(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter text\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gingerit.gingerit'"
     ]
    }
   ],
   "source": [
    "from gingerit.gingerit import GingerIt\n",
    "\n",
    "st.title('Spell Corrector and Grammar Checker')\n",
    "text=st.text_input(label='Enter text')\n",
    "parser = GingerIt()\n",
    "res=parser.parse(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b664003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of lines in active voice: 1.20\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def check_voice(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    # Check for active/passive voice\n",
    "    active_voice = sum(1 for token in doc if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB')\n",
    "    passive_voice = sum(1 for token in doc if token.dep_ == 'nsubjpass' and token.head.pos_ == 'VERB')\n",
    "    if active_voice > passive_voice:\n",
    "        return \"Active Voice\"\n",
    "    elif passive_voice > active_voice:\n",
    "        return \"Passive Voice\"\n",
    "    else:\n",
    "        return \"Unable to determine voice\"\n",
    "\n",
    "def calculate_active_voice_percentage(resume_lines):\n",
    "    total_lines = len(resume_lines)\n",
    "    active_voice_lines = 0\n",
    "\n",
    "    for line in resume_lines:\n",
    "        voice = check_voice(line)\n",
    "        if voice == \"Active Voice\":\n",
    "            active_voice_lines += 1\n",
    "\n",
    "    active_voice_percentage = (active_voice_lines * 3) / total_lines \n",
    "    return active_voice_percentage\n",
    "\n",
    "# Example usage\n",
    "resume_lines = [\n",
    "    \"I am a highly skilled software developer with experience in Python and Java.\",\n",
    "    \"The project was completed by our team on time.\",\n",
    "    \"I have developed numerous applications using cutting-edge technologies.\",\n",
    "    \"Responsibilities included code review and debugging.\",\n",
    "    \"The software was designed to meet the client's requirements.\"\n",
    "]\n",
    "\n",
    "active_voice_percentage = calculate_active_voice_percentage(resume_lines)\n",
    "print(f\"Percentage of lines in active voice: {active_voice_percentage:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
